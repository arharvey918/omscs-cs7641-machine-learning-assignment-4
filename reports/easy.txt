Grid world has 19 states
/////Easy Grid World Analysis/////

This is your grid world:
[0,0,0,0,0]
[0,0,0,0,0]
[0,1,1,1,0]
[0,1,1,1,0]
[0,0,0,0,0]

This is your optimal policy:
num of rows in policy is 5
[>,>,>,>,<]
[>,>,>,>,^]
[^,*,*,*,^]
[^,*,*,*,^]
[^,>,>,>,^]

Num generated: 260; num unique: 19
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,288,248,16,15,10,16,9,15,11,10,15,11,13,10,12,13,11,15,14,11,15,9,11,22,12,15,9,12,13,10,14,10,12,20,11,16,14,11,11,11,20,14,17,20,22,13,9,11,15,12,19,11,20,13,16,13,15,9,18,16,14,13,11,12,12,12,9,21,10,13,12,23,22,9,12,13,11,13,12,10,19,9,12,13,20,11,13,15,20,13,15,10,16,12,17,15,17,11,11,14
Policy Iteration,39,15,51,12,14,12,9,10,11,13,14,9,9,14,14,13,22,11,14,12,15,18,11,28,10,19,13,13,20,11,14,10,10,13,12,11,13,17,10,10,11,23,13,9,16,14,13,10,17,12,9,11,16,17,12,17,12,17,12,9,16,9,11,17,19,13,11,18,11,12,10,9,13,20,13,10,10,15,14,9,12,20,20,12,20,21,16,11,11,13,13,11,15,13,13,11,18,11,9,13
Q Learning,80,111,34,15,12,37,46,53,16,30,31,14,19,202,33,23,113,31,23,15,21,30,34,38,29,15,21,10,14,12,22,63,24,27,23,12,18,15,19,17,17,12,17,46,15,51,15,15,16,14,33,15,21,15,12,64,15,9,11,11,37,11,19,31,21,18,12,21,13,31,32,17,19,23,19,9,36,15,22,11,15,28,22,12,14,11,37,20,26,16,10,16,24,14,100,23,17,15,25,17

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,60,4,9,42,17,23,5,9,7,8,9,9,21,11,34,21,10,34,23,14,41,18,17,34,16,13,19,15,17,23,28,61,21,19,19,19,21,55,59,51,35,24,15,15,56,91,91,82,63,34,46,52,23,53,27,16,18,25,18,26,17,19,31,44,58,38,22,23,26,21,24,25,31,19,20,17,16,21,24,19,21,19,20,15,21,17,18,19,22,19,20,17,15,17,17,18,19,16,16,16
Policy Iteration,53,27,19,14,11,41,11,12,13,14,20,23,17,20,56,62,56,21,33,22,57,39,25,26,27,42,26,51,50,25,28,27,27,30,41,31,36,34,31,33,36,36,38,39,41,34,37,37,40,37,38,38,47,44,37,26,35,25,26,33,35,45,47,55,57,54,48,46,47,52,56,39,29,33,39,39,31,35,51,60,59,58,41,35,37,59,59,67,70,74,77,62,63,63,68,74,75,67,41,40
Q Learning,54,60,24,20,12,8,9,24,15,26,8,5,6,17,22,6,11,7,8,7,8,11,13,14,9,19,28,12,7,9,13,13,18,22,14,28,18,14,19,10,7,7,6,13,23,9,9,14,9,9,7,8,7,8,9,10,10,10,12,8,26,9,9,13,12,11,9,12,9,9,14,17,14,13,11,12,11,28,45,36,14,13,12,18,11,22,41,18,14,14,13,14,18,15,15,17,16,15,14,18

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-186.0,-146.0,86.0,87.0,92.0,86.0,93.0,87.0,91.0,92.0,87.0,91.0,89.0,92.0,90.0,89.0,91.0,87.0,88.0,91.0,87.0,93.0,91.0,80.0,90.0,87.0,93.0,90.0,89.0,92.0,88.0,92.0,90.0,82.0,91.0,86.0,88.0,91.0,91.0,91.0,82.0,88.0,85.0,82.0,80.0,89.0,93.0,91.0,87.0,90.0,83.0,91.0,82.0,89.0,86.0,89.0,87.0,93.0,84.0,86.0,88.0,89.0,91.0,90.0,90.0,90.0,93.0,81.0,92.0,89.0,90.0,79.0,80.0,93.0,90.0,89.0,91.0,89.0,90.0,92.0,83.0,93.0,90.0,89.0,82.0,91.0,89.0,87.0,82.0,89.0,87.0,92.0,86.0,90.0,85.0,87.0,85.0,91.0,91.0,88.0
Policy Iteration Rewards,63.0,87.0,51.0,90.0,88.0,90.0,93.0,92.0,91.0,89.0,88.0,93.0,93.0,88.0,88.0,89.0,80.0,91.0,88.0,90.0,87.0,84.0,91.0,74.0,92.0,83.0,89.0,89.0,82.0,91.0,88.0,92.0,92.0,89.0,90.0,91.0,89.0,85.0,92.0,92.0,91.0,79.0,89.0,93.0,86.0,88.0,89.0,92.0,85.0,90.0,93.0,91.0,86.0,85.0,90.0,85.0,90.0,85.0,90.0,93.0,86.0,93.0,91.0,85.0,83.0,89.0,91.0,84.0,91.0,90.0,92.0,93.0,89.0,82.0,89.0,92.0,92.0,87.0,88.0,93.0,90.0,82.0,82.0,90.0,82.0,81.0,86.0,91.0,91.0,89.0,89.0,91.0,87.0,89.0,89.0,91.0,84.0,91.0,93.0,89.0
Q Learning Rewards,22.0,-9.0,68.0,87.0,90.0,65.0,56.0,49.0,86.0,72.0,71.0,88.0,83.0,-100.0,69.0,79.0,-11.0,71.0,79.0,87.0,81.0,72.0,68.0,64.0,73.0,87.0,81.0,92.0,88.0,90.0,80.0,39.0,78.0,75.0,79.0,90.0,84.0,87.0,83.0,85.0,85.0,90.0,85.0,56.0,87.0,51.0,87.0,87.0,86.0,88.0,69.0,87.0,81.0,87.0,90.0,38.0,87.0,93.0,91.0,91.0,65.0,91.0,83.0,71.0,81.0,84.0,90.0,81.0,89.0,71.0,70.0,85.0,83.0,79.0,83.0,93.0,66.0,87.0,80.0,91.0,87.0,74.0,80.0,90.0,88.0,91.0,65.0,82.0,76.0,86.0,92.0,86.0,78.0,88.0,2.0,79.0,85.0,87.0,77.0,85.0

Value Iteration: 
Policy Iteration: 15 passes
Q-Learning: 15 passes



**********************AVERAGES
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,164.76,123.54,14.07,14.14,14.07,14.53,13.49,13.01,13.58,13.6,13.76,13.04,13.31,13.64,13.71,13.56,13.18,13.79,14.37,13.13,13.64,13.02,13.38,14.45,13.47,13.65,13.29,13.23,13.22,13.79,13.87,14.46,13.19,13.67,13.37,12.84,13.37,13.63,14.13,13.93,13.69,13.61,13.28,13.36,13.67,13.58,13.21,13.62,14.11,13.52,13.7,13.36,13.49,13.72,13.41,13.48,13.22,13.78,12.97,13.9,13.55,12.96,13.68,13.45,13.76,12.93,13.46,13.38,13.2,13.32,13.79,12.98,13.64,14.02,14.2,13.34,13.82,13.52,14.16,13.49,13.57,13.87,14.07,13.15,13.16,13.37,13.89,13.23,13.63,13.93,13.27,13.43,12.72,13.36,13.31,13.37,13.17,13.58,13.82,13.22
Policy Iteration,147.56,95.78,24.43,13.36,13.95,13.96,13.71,13.75,13.62,14.82,13.42,14.15,13.69,13.02,13.56,13.6,13.4,13.21,13.19,13.0,13.4,13.58,13.52,12.81,13.7,13.73,13.05,13.23,13.46,13.87,13.24,14.28,13.49,13.23,13.79,14.04,12.74,13.68,13.02,13.62,13.6,13.66,13.56,13.35,12.74,13.84,13.17,13.14,13.65,13.2,12.86,13.45,13.61,13.17,13.59,13.11,12.73,13.32,14.11,13.92,13.11,13.59,13.59,13.62,13.35,13.1,13.26,14.03,13.42,14.31,13.23,13.41,13.79,13.67,13.85,12.89,14.13,13.38,13.25,13.75,13.49,13.68,13.86,13.98,13.4,12.93,12.97,13.63,13.88,13.36,13.71,13.68,14.25,13.81,13.72,13.62,13.68,13.25,13.21,13.93
Q Learning,81.48,61.93,64.58,56.63,43.51,47.94,40.9,41.62,33.35,35.48,32.34,38.6,30.71,28.89,30.44,28.06,33.15,31.46,27.17,25.33,28.24,27.71,26.1,24.05,26.37,24.9,25.41,25.3,27.64,23.31,30.03,26.65,24.67,23.33,25.14,21.18,29.54,24.94,26.95,27.15,26.47,26.93,27.91,24.28,24.95,25.17,29.14,21.6,27.54,23.15,24.45,24.12,25.81,24.89,25.04,22.44,24.84,23.74,25.71,23.8,27.56,24.88,25.21,26.3,22.03,21.91,27.02,21.24,23.81,23.55,22.83,25.77,24.08,27.7,25.51,24.39,21.35,25.0,24.52,23.53,22.47,25.53,24.99,24.91,25.23,26.57,21.99,25.28,21.59,24.61,25.47,28.91,22.03,22.45,23.65,22.47,24.49,23.78,25.1,27.41

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,3.18,2.24,0.32,1.12,1.58,1.36,1.25,1.39,1.6,2.26,2.32,2.28,2.39,2.7,2.93,3.47,3.45,3.53,3.6,3.57,4.01,4.51,4.56,4.71,4.89,5.13,5.43,5.67,5.64,6.02,6.01,6.42,6.5,6.85,6.74,6.89,7.17,7.35,7.67,7.59,7.72,8.0,8.29,8.41,8.73,8.65,8.91,9.09,9.27,9.5,9.9,10.1,10.02,10.12,10.33,10.77,10.88,11.03,11.17,11.32,11.32,11.63,11.99,12.38,12.28,12.63,12.53,13.06,13.36,13.3,13.36,13.48,13.97,14.27,14.2,14.39,14.38,14.71,14.96,15.08,15.09,15.36,15.72,15.45,16.03,16.34,16.34,16.43,16.94,16.89,17.15,17.42,17.44,17.81,18.18,18.25,18.34,18.61,18.96,19.16
Policy Iteration,1.98,1.73,1.15,1.16,1.98,2.27,2.3,3.09,3.26,3.5,4.28,4.49,4.59,5.23,5.53,5.99,6.56,6.81,7.06,7.6,7.94,8.33,8.72,8.75,9.15,9.91,9.66,10.16,10.75,11.14,11.43,11.89,12.13,12.65,13.18,13.7,14.0,14.23,14.72,14.78,15.48,15.86,15.93,16.41,16.99,17.46,18.04,18.4,18.72,19.59,19.36,19.79,19.97,20.31,20.56,20.95,21.26,22.09,22.23,22.72,22.86,23.14,23.22,23.65,23.65,24.29,24.91,25.27,25.54,26.14,26.86,27.2,27.62,28.18,28.16,28.79,28.92,29.06,29.35,29.88,30.36,30.97,30.76,31.38,31.57,31.87,32.46,32.57,31.87,32.81,32.99,33.77,34.68,35.08,35.47,35.13,35.15,35.43,36.03,36.02
Q Learning,0.06,0.1,0.3,0.44,0.66,0.88,0.93,1.15,1.15,1.28,1.45,1.4,1.56,1.61,1.69,1.89,2.01,2.03,2.11,2.17,2.37,2.39,2.49,2.45,2.71,2.69,2.96,2.85,3.05,2.91,3.22,3.07,3.22,3.37,3.56,3.49,3.72,3.65,3.59,3.87,3.98,4.29,4.2,4.13,4.12,4.25,4.35,4.25,4.63,4.41,4.55,4.57,4.82,4.64,4.78,4.91,5.04,4.91,4.95,5.09,5.23,5.38,5.34,5.84,5.77,5.67,5.78,5.78,5.86,6.11,5.97,5.84,6.23,6.2,6.51,6.4,6.54,6.62,6.54,6.68,6.83,7.23,6.99,6.79,7.13,7.34,7.18,7.14,7.15,7.45,7.55,7.78,7.64,7.89,7.99,8.11,7.91,8.37,8.25,8.21

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-62.76,-21.54,87.93,87.86,87.93,87.47,88.51,88.99,88.42,88.4,88.24,88.96,88.69,88.36,88.29,88.44,88.82,88.21,87.63,88.87,88.36,88.98,88.62,87.55,88.53,88.35,88.71,88.77,88.78,88.21,88.13,87.54,88.81,88.33,88.63,89.16,88.63,88.37,87.87,88.07,88.31,88.39,88.72,88.64,88.33,88.42,88.79,88.38,87.89,88.48,88.3,88.64,88.51,88.28,88.59,88.52,88.78,88.22,89.03,88.1,88.45,89.04,88.32,88.55,88.24,89.07,88.54,88.62,88.8,88.68,88.21,89.02,88.36,87.98,87.8,88.66,88.18,88.48,87.84,88.51,88.43,88.13,87.93,88.85,88.84,88.63,88.11,88.77,88.37,88.07,88.73,88.57,89.28,88.64,88.69,88.63,88.83,88.42,88.18,88.78
Policy Iteration Rewards,-45.56,6.22,77.57,88.64,88.05,88.04,88.29,88.25,88.38,87.18,88.58,87.85,88.31,88.98,88.44,88.4,88.6,88.79,88.81,89.0,88.6,88.42,88.48,89.19,88.3,88.27,88.95,88.77,88.54,88.13,88.76,87.72,88.51,88.77,88.21,87.96,89.26,88.32,88.98,88.38,88.4,88.34,88.44,88.65,89.26,88.16,88.83,88.86,88.35,88.8,89.14,88.55,88.39,88.83,88.41,88.89,89.27,88.68,87.89,88.08,88.89,88.41,88.41,88.38,88.65,88.9,88.74,87.97,88.58,87.69,88.77,88.59,88.21,88.33,88.15,89.11,87.87,88.62,88.75,88.25,88.51,88.32,88.14,88.02,88.6,89.07,89.03,88.37,88.12,88.64,88.29,88.32,87.75,88.19,88.28,88.38,88.32,88.75,88.79,88.07
Q Learning Rewards,20.52,40.07,37.42,45.37,58.49,54.06,61.1,60.38,68.65,66.52,69.66,63.4,71.29,73.11,71.56,73.94,68.85,70.54,74.83,76.67,73.76,74.29,75.9,77.95,75.63,77.1,76.59,76.7,74.36,78.69,71.97,75.35,77.33,78.67,76.86,80.82,72.46,77.06,75.05,74.85,75.53,75.07,74.09,77.72,77.05,76.83,72.86,80.4,74.46,78.85,77.55,77.88,76.19,77.11,76.96,79.56,77.16,78.26,76.29,78.2,74.44,77.12,76.79,75.7,79.97,80.09,74.98,80.76,78.19,78.45,79.17,76.23,77.92,74.3,76.49,77.61,80.65,77.0,77.48,78.47,79.53,76.47,77.01,77.09,76.77,75.43,80.01,76.72,80.41,77.39,76.53,73.09,79.97,79.55,78.35,79.53,77.51,78.22,76.9,74.59



#################################QLEARNING

//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000
Value Iteration,
Policy Iteration,
Q Learning,17.25,15.58,15.04,14.67,15.52,16.25,15.81,15.16,15.71,14.84,14.41,14.72,15.17,15.75,14.91,15.3,15.34,14.92,15.08,14.74

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000
Value Iteration,
Policy Iteration,
Q Learning,51.01,93.54,138.38,181.74,222.65,261.36,305.97,346.94,390.29,433.11,475.24,520.41,559.64,601.93,645.62,683.15,745.44,772.55,814.19,853.03

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,84.75,86.42,86.96,87.33,86.48,85.75,86.19,86.84,86.29,87.16,87.59,87.28,86.83,86.25,87.09,86.7,86.66,87.08,86.92,87.26

#################
MORE QLEARNING

//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,
Policy Iteration,
Q Learning,44.9,24.9,25.3,18.4,22.8,16.4,18.6,15.4,16.5,19.2,18.5,13.4,16.6,17.7,16.3,17.5,15.3,13.4,18.5,17.0,14.4,17.2,14.6,15.8,16.0,14.8,17.1,16.9,14.3,17.1,18.8,15.8,13.7,13.0,15.4,14.7,16.2,18.6,15.1,15.8,17.9,14.9,14.6,13.2,13.7,16.5,17.1,14.8,17.6,14.4,16.0,14.6,14.4,16.6,16.0,15.9,18.2,13.3,15.7,13.7,14.3,15.4,17.2,16.4,16.1,16.4,15.8,15.9,13.6,14.6,16.1,18.7,18.3,17.7,15.5,14.8,13.1,15.9,16.2,16.4,14.0,14.3,18.7,17.1,17.1,15.8,14.7,14.5,15.1,18.1,18.5,15.9,15.7,12.4,16.6,15.1,16.5,15.2,17.2,14.2

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,
Policy Iteration,
Q Learning,14.9,7.2,11.3,8.2,7.7,9.5,8.6,7.4,8.6,10.9,11.8,12.1,12.9,9.9,10.9,11.8,11.8,13.3,15.8,15.4,15.5,16.9,14.5,15.6,14.8,16.0,16.7,18.1,19.1,18.0,19.1,20.3,19.4,19.9,20.8,21.9,20.2,20.7,21.1,21.0,20.8,22.7,22.1,22.9,25.4,25.7,25.5,27.4,28.8,29.6,29.6,28.9,30.4,31.5,30.4,28.4,29.9,28.8,30.0,29.1,32.3,31.0,31.0,32.7,32.4,34.3,34.3,33.6,36.3,34.9,35.3,37.2,35.8,37.2,40.1,39.7,40.4,40.5,37.9,40.4,39.0,39.5,40.1,42.1,45.1,43.0,42.8,42.3,44.0,43.4,45.0,44.9,44.0,45.0,45.8,48.5,49.6,45.4,49.3,49.4

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,57.1,77.1,76.7,83.6,79.2,85.6,83.4,86.6,85.5,82.8,83.5,88.6,85.4,84.3,85.7,84.5,86.7,88.6,83.5,85.0,87.6,84.8,87.4,86.2,86.0,87.2,84.9,85.1,87.7,84.9,83.2,86.2,88.3,89.0,86.6,87.3,85.8,83.4,86.9,86.2,84.1,87.1,87.4,88.8,88.3,85.5,84.9,87.2,84.4,87.6,86.0,87.4,87.6,85.4,86.0,86.1,83.8,88.7,86.3,88.3,87.7,86.6,84.8,85.6,85.9,85.6,86.2,86.1,88.4,87.4,85.9,83.3,83.7,84.3,86.5,87.2,88.9,86.1,85.8,85.6,88.0,87.7,83.3,84.9,84.9,86.2,87.3,87.5,86.9,83.9,83.5,86.1,86.3,89.6,85.4,86.9,85.5,86.8,84.8,87.8

