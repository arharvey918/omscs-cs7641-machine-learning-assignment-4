Grid world has 279 states
/////Hard Grid World Analysis/////

This is your grid world:
[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,0]
[0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0]
[0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0]
[0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0]
[0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0]
[0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0]
[0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0]
[0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,0,0]
[0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,1,0,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,0,0]
[0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]
[0,1,0,1,1,1,1,1,0,1,1,1,1,1,0,0,0,1,0,0]
[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]
[0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0]
[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0]
[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]

This is your optimal policy:
num of rows in policy is 20
[>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,v]
[^,*,^,*,*,*,*,*,*,^,*,*,*,*,*,*,*,*,^,^]
[^,*,^,*,>,>,>,>,>,^,>,>,>,>,>,>,>,>,^,^]
[^,*,^,*,>,>,>,^,^,*,>,>,>,^,^,*,>,>,^,^]
[^,*,^,*,^,>,^,^,^,*,^,>,^,^,^,*,>,^,^,^]
[^,*,^,*,^,^,^,^,^,*,^,^,^,^,^,*,*,*,^,^]
[^,*,^,*,^,^,^,^,^,*,^,^,^,^,^,*,>,>,^,^]
[^,*,^,*,^,^,^,^,^,*,^,^,^,^,^,*,>,>,^,^]
[^,*,^,*,^,>,^,^,^,*,^,>,^,^,^,*,^,^,^,^]
[^,*,^,*,*,*,*,*,^,*,*,*,*,*,*,*,^,^,^,^]
[^,*,^,*,>,>,>,>,>,>,>,>,>,>,>,>,^,^,^,^]
[^,*,^,*,^,*,*,*,*,*,*,*,^,*,*,*,*,*,^,^]
[^,*,^,*,^,>,>,>,>,>,>,>,^,<,<,<,v,*,^,^]
[^,*,^,*,*,*,*,*,^,*,*,*,*,*,^,>,v,*,^,^]
[^,*,>,>,>,>,>,>,^,<,>,>,>,>,>,>,v,*,^,^]
[^,*,^,*,*,*,*,*,*,*,*,*,*,*,*,*,>,>,^,^]
[^,*,^,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,^,^]
[^,*,^,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,^,^]
[>,>,^,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,^,^]
[>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,>,^,*]


Num generated: 4040; num unique: 279
//Aggregate Analysis//

Value Iteration: 1-100
Policy Iteration: Passes: 46
Q Learning:

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,1513,3512,35407,8927,60182,691,428,433,566,591,187,545,467,120,215,644,77,168,87,82,72,63,70,75,60,60,55,74,55,64,53,69,76,52,61,71,66,47,74,71,64,64,59,54,55,59,67,63,66,70,54,77,54,69,59,57,81,55,50,72,54,61,68,77,58,58,59,78,60,59,59,70,57,64,53,59,59,78,52,80,57,67,62,89,63,57,62,78,76,63,63,88,70,62,50,61,70,52,72,54
Policy Iteration,13675,719,43608,50608,1093,2632,1317,4695,112667,37036,14071,33447,1878,2359,2915,1300,378,58,55,70,62,81,77,88,62,52,103,60,68,60,48,56,69,69,56,57,56,55,65,61,60,57,52,65,63,66,65,54,64,65,61,55,57,71,69,70,68,73,56,69,70,72,65,59,58,72,59,64,48,99,48,59,53,72,56,68,59,57,60,70,64,65,64,62,62,68,64,65,56,57,46,61,63,67,80,58,58,60,71,70
Q Learning,3214,589,1846,404,790,2463,1566,368,696,1400,801,430,719,1396,335,1130,221,774,656,345,388,600,290,262,794,445,502,680,344,245,193,217,763,339,315,372,426,273,243,456,243,542,227,347,275,319,467,374,132,258,194,249,588,165,336,208,294,258,171,207,229,196,714,410,106,124,204,227,507,232,292,143,277,191,153,326,161,200,150,335,500,382,356,206,164,192,314,136,85,163,283,213,283,120,257,189,290,109,165,354

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,153,44,56,29,29,26,28,32,34,38,48,50,53,55,78,65,64,71,74,74,87,101,84,91,91,94,145,132,114,109,117,124,128,123,135,139,138,138,145,151,152,160,166,157,169,165,182,177,180,191,187,186,207,193,215,203,209,225,215,221,234,228,241,235,247,243,247,255,254,273,264,292,275,283,275,286,283,293,289,303,298,312,300,316,310,323,321,329,322,336,335,333,347,340,353,363,394,383,362,371
Policy Iteration,199,69,52,61,39,44,47,136,56,128,71,222,81,91,103,100,105,120,123,125,151,172,166,163,177,164,186,175,191,182,198,256,213,211,221,233,230,244,283,244,256,265,275,333,284,287,287,294,367,310,309,341,330,326,374,345,344,366,421,371,374,378,382,391,405,408,400,416,417,424,422,433,432,445,455,459,468,468,478,481,488,491,504,510,509,514,530,527,530,548,555,561,561,568,572,573,589,599,602,605
Q Learning,223,214,189,94,110,94,154,166,93,71,108,63,61,78,50,68,118,90,133,122,115,138,136,152,120,119,135,75,79,77,75,95,104,93,81,81,73,89,81,81,97,97,88,97,95,119,93,99,113,100,102,100,98,99,92,108,110,103,93,101,99,103,110,113,110,115,113,106,118,114,119,106,113,103,117,114,116,116,120,118,126,124,125,121,116,134,135,133,130,131,138,135,130,124,127,133,132,117,134,155

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-1411.0,-3410.0,-35305.0,-8825.0,-60080.0,-589.0,-326.0,-331.0,-464.0,-489.0,-85.0,-443.0,-365.0,-18.0,-113.0,-542.0,25.0,-66.0,15.0,20.0,30.0,39.0,32.0,27.0,42.0,42.0,47.0,28.0,47.0,38.0,49.0,33.0,26.0,50.0,41.0,31.0,36.0,55.0,28.0,31.0,38.0,38.0,43.0,48.0,47.0,43.0,35.0,39.0,36.0,32.0,48.0,25.0,48.0,33.0,43.0,45.0,21.0,47.0,52.0,30.0,48.0,41.0,34.0,25.0,44.0,44.0,43.0,24.0,42.0,43.0,43.0,32.0,45.0,38.0,49.0,43.0,43.0,24.0,50.0,22.0,45.0,35.0,40.0,13.0,39.0,45.0,40.0,24.0,26.0,39.0,39.0,14.0,32.0,40.0,52.0,41.0,32.0,50.0,30.0,48.0
Policy Iteration Rewards,-13573.0,-617.0,-43506.0,-50506.0,-991.0,-2530.0,-1215.0,-4593.0,-112565.0,-36934.0,-13969.0,-33345.0,-1776.0,-2257.0,-2813.0,-1198.0,-276.0,44.0,47.0,32.0,40.0,21.0,25.0,14.0,40.0,50.0,-1.0,42.0,34.0,42.0,54.0,46.0,33.0,33.0,46.0,45.0,46.0,47.0,37.0,41.0,42.0,45.0,50.0,37.0,39.0,36.0,37.0,48.0,38.0,37.0,41.0,47.0,45.0,31.0,33.0,32.0,34.0,29.0,46.0,33.0,32.0,30.0,37.0,43.0,44.0,30.0,43.0,38.0,54.0,3.0,54.0,43.0,49.0,30.0,46.0,34.0,43.0,45.0,42.0,32.0,38.0,37.0,38.0,40.0,40.0,34.0,38.0,37.0,46.0,45.0,56.0,41.0,39.0,35.0,22.0,44.0,44.0,42.0,31.0,32.0
Q Learning Rewards,-3112.0,-487.0,-1744.0,-302.0,-688.0,-2361.0,-1464.0,-266.0,-594.0,-1298.0,-699.0,-328.0,-617.0,-1294.0,-233.0,-1028.0,-119.0,-672.0,-554.0,-243.0,-286.0,-498.0,-188.0,-160.0,-692.0,-343.0,-400.0,-578.0,-242.0,-143.0,-91.0,-115.0,-661.0,-237.0,-213.0,-270.0,-324.0,-171.0,-141.0,-354.0,-141.0,-440.0,-125.0,-245.0,-173.0,-217.0,-365.0,-272.0,-30.0,-156.0,-92.0,-147.0,-486.0,-63.0,-234.0,-106.0,-192.0,-156.0,-69.0,-105.0,-127.0,-94.0,-612.0,-308.0,-4.0,-22.0,-102.0,-125.0,-405.0,-130.0,-190.0,-41.0,-175.0,-89.0,-51.0,-224.0,-59.0,-98.0,-48.0,-233.0,-398.0,-280.0,-254.0,-104.0,-62.0,-90.0,-212.0,-34.0,17.0,-61.0,-181.0,-111.0,-181.0,-18.0,-155.0,-87.0,-188.0,-7.0,-63.0,-252.0




************************************************AVERAGES
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,10146.3,13828.83,36964.54,134131.98,52920.06,50793.13,36310.99,33602.5,41205.84,1456.06,2926.39,2679.62,3264.1,2379.84,1172.98,315.43,152.69,119.41,97.37,68.97,63.56,64.56,63.99,65.18,62.73,65.26,63.45,65.55,65.44,64.46,63.92,66.08,63.85,65.86,64.32,64.34,64.05,64.68,64.82,62.13,62.26,64.42,62.39,64.2,63.23,62.76,63.23,63.93,63.7,64.28,63.7,63.12,63.91,64.47,65.68,62.81,64.21,63.75,62.9,62.65,63.91,63.01,63.62,61.82,63.92,63.44,62.89,63.03,63.45,65.15,63.23,63.2,65.05,64.24,61.88,62.66,62.98,63.19,64.93,64.91,64.06,66.17,64.01,62.49,63.6,63.92,63.82,64.49,65.01,63.81,64.21,63.25,62.05,63.82,63.89,63.02,63.4,63.7,61.69,63.93
Policy Iteration,24348.35,2409.87,43492.31,121647.51,21569.57,15905.74,7014.81,14049.68,19108.6,52368.73,12094.38,30075.87,7103.16,8612.78,1593.77,1132.74,941.02,103.0,77.99,76.41,64.48,65.46,65.2,62.9,64.66,65.6,64.67,66.43,63.02,63.87,63.29,65.13,64.56,63.17,61.77,64.96,63.75,64.45,63.71,62.8,62.77,61.28,63.68,63.01,64.6,62.53,63.4,63.69,64.12,62.34,65.37,63.03,63.53,63.95,64.84,63.22,64.53,65.09,63.72,65.29,64.26,62.9,63.69,63.24,63.4,63.23,62.67,64.07,64.45,63.65,63.25,63.25,63.85,63.91,64.1,63.87,62.58,63.29,64.0,62.23,62.96,64.05,64.02,64.16,64.53,63.97,65.05,62.56,61.97,62.67,62.96,63.77,62.36,63.49,63.65,64.03,62.8,62.89,64.16,64.08
Q Learning,2303.8,1846.84,1594.62,1273.63,1391.74,1146.97,998.3,972.0,934.87,803.92,733.01,792.77,711.0,691.18,648.8,742.66,632.96,634.08,640.18,534.52,555.68,449.27,427.5,450.02,450.38,459.59,420.64,448.33,494.74,406.69,417.99,393.32,396.67,357.63,350.88,347.97,373.97,353.78,382.08,348.17,334.84,341.42,324.96,354.72,315.04,280.6,301.07,297.74,325.53,284.52,260.87,285.19,290.9,295.33,275.62,277.61,290.76,252.1,256.14,256.86,255.72,270.83,258.14,254.45,235.64,232.95,224.49,252.95,242.7,240.18,230.47,204.29,235.01,235.67,215.83,213.85,243.2,234.66,229.55,230.62,243.62,191.83,215.57,223.18,197.93,200.07,205.73,203.6,202.07,213.04,235.16,195.66,203.62,191.44,190.08,207.46,204.37,193.24,202.29,190.5

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,127.29,178.29,371.3,111.68,296.02,257.1,194.76,282.29,142.27,60.46,78.12,78.43,89.82,83.57,73.61,64.97,66.31,69.26,72.24,75.73,79.69,83.29,85.59,89.81,92.33,96.68,100.56,104.13,106.72,110.43,114.65,118.35,121.27,124.57,128.45,132.62,134.64,138.67,142.53,145.94,149.22,153.31,157.09,159.87,163.54,167.37,170.03,175.55,177.41,181.44,184.38,187.75,191.77,194.88,199.32,202.59,206.73,209.13,212.88,217.17,220.29,224.27,226.5,231.37,233.69,239.32,242.54,246.86,249.97,252.28,256.19,259.8,263.54,265.87,270.4,273.63,277.38,280.16,283.74,287.77,290.63,295.36,296.9,301.64,304.33,308.67,312.86,315.0,319.33,321.31,325.78,328.39,333.02,335.78,339.33,344.55,346.01,349.76,354.0,356.24
Policy Iteration,306.4,45.4,291.74,261.4,143.21,237.37,134.8,227.42,288.5,583.87,221.99,449.82,169.38,194.18,112.16,109.95,114.04,108.88,114.51,120.18,125.84,131.47,137.58,143.15,149.48,154.29,160.63,165.85,172.31,177.31,183.88,189.06,195.21,201.12,206.79,212.76,218.0,224.05,229.67,235.33,241.46,246.9,252.91,258.32,264.05,269.47,275.27,281.46,287.81,292.71,298.19,303.77,310.0,314.86,321.29,326.19,331.93,338.52,343.57,349.01,355.44,359.9,366.0,371.89,377.65,382.95,389.99,395.4,400.36,406.5,411.75,417.7,422.74,428.97,433.83,439.98,445.76,451.1,457.13,462.55,468.15,474.0,479.98,485.69,491.18,496.16,503.03,508.23,514.48,519.45,525.47,530.63,536.3,543.39,547.95,553.96,559.97,565.42,570.91,576.58
Q Learning,14.03,18.37,22.27,27.32,31.4,33.76,38.59,39.98,45.01,45.89,49.04,51.19,54.34,56.98,57.49,60.1,61.02,63.33,65.59,67.54,68.82,70.19,70.4,72.57,74.5,74.14,76.63,78.32,79.7,80.08,80.87,82.93,81.87,86.64,85.99,87.92,90.26,92.66,94.45,95.89,94.66,95.72,95.25,96.92,98.25,102.01,103.58,106.55,104.04,104.73,105.28,106.22,107.74,110.81,112.18,111.18,109.77,109.07,111.43,110.75,113.99,114.63,113.36,113.39,115.24,117.98,116.46,117.88,117.45,118.18,119.75,119.97,122.41,121.91,121.94,125.1,125.6,126.78,128.54,129.75,130.83,131.14,128.91,131.32,131.6,131.62,135.05,133.04,134.55,136.78,136.94,140.13,139.24,136.0,135.97,139.59,139.55,136.86,139.7,139.86

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-10044.3,-13726.83,-36862.54,-134029.98,-52818.06,-50691.13,-36208.99,-33500.5,-41103.84,-1354.06,-2824.39,-2577.62,-3162.1,-2277.84,-1070.98,-213.43,-50.69,-17.41,4.63,33.03,38.44,37.44,38.01,36.82,39.27,36.74,38.55,36.45,36.56,37.54,38.08,35.92,38.15,36.14,37.68,37.66,37.95,37.32,37.18,39.87,39.74,37.58,39.61,37.8,38.77,39.24,38.77,38.07,38.3,37.72,38.3,38.88,38.09,37.53,36.32,39.19,37.79,38.25,39.1,39.35,38.09,38.99,38.38,40.18,38.08,38.56,39.11,38.97,38.55,36.85,38.77,38.8,36.95,37.76,40.12,39.34,39.02,38.81,37.07,37.09,37.94,35.83,37.99,39.51,38.4,38.08,38.18,37.51,36.99,38.19,37.79,38.75,39.95,38.18,38.11,38.98,38.6,38.3,40.31,38.07
Policy Iteration Rewards,-24246.35,-2307.87,-43390.31,-121545.51,-21467.57,-15803.74,-6912.81,-13947.68,-19006.6,-52266.73,-11992.38,-29973.87,-7001.16,-8510.78,-1491.77,-1030.74,-839.02,-1.0,24.01,25.59,37.52,36.54,36.8,39.1,37.34,36.4,37.33,35.57,38.98,38.13,38.71,36.87,37.44,38.83,40.23,37.04,38.25,37.55,38.29,39.2,39.23,40.72,38.32,38.99,37.4,39.47,38.6,38.31,37.88,39.66,36.63,38.97,38.47,38.05,37.16,38.78,37.47,36.91,38.28,36.71,37.74,39.1,38.31,38.76,38.6,38.77,39.33,37.93,37.55,38.35,38.75,38.75,38.15,38.09,37.9,38.13,39.42,38.71,38.0,39.77,39.04,37.95,37.98,37.84,37.47,38.03,36.95,39.44,40.03,39.33,39.04,38.23,39.64,38.51,38.35,37.97,39.2,39.11,37.84,37.92
Q Learning Rewards,-2201.8,-1744.84,-1492.62,-1171.63,-1289.74,-1044.97,-896.3,-870.0,-832.87,-701.92,-631.01,-690.77,-609.0,-589.18,-546.8,-640.66,-530.96,-532.08,-538.18,-432.52,-453.68,-347.27,-325.5,-348.02,-348.38,-357.59,-318.64,-346.33,-392.74,-304.69,-315.99,-291.32,-294.67,-255.63,-248.88,-245.97,-271.97,-251.78,-280.08,-246.17,-232.84,-239.42,-222.96,-252.72,-213.04,-178.6,-199.07,-195.74,-223.53,-182.52,-158.87,-183.19,-188.9,-193.33,-173.62,-175.61,-188.76,-150.1,-154.14,-154.86,-153.72,-168.83,-156.14,-152.45,-133.64,-130.95,-122.49,-150.95,-140.7,-138.18,-128.47,-102.29,-133.01,-133.67,-113.83,-111.85,-141.2,-132.66,-127.55,-128.62,-141.62,-89.83,-113.57,-121.18,-95.93,-98.07,-103.73,-101.6,-100.07,-111.04,-133.16,-93.66,-101.62,-89.44,-88.08,-105.46,-102.37,-91.24,-100.29,-88.5



#######################################3 Q AVERAGES
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,
Policy Iteration,
Q Learning,1441.6,1067.6,584.8,809.6,1033.2,738.6,376.6,513.2,339.8,494.6,453.6,464.2,393.0,494.4,486.6,302.0,277.2,278.8,280.6,240.0,229.2,173.0,331.0,213.8,284.2,217.2,235.0,195.6,168.6,153.6,269.6,195.0,197.0,156.4,171.0,123.6,154.4,202.6,157.6,131.8,123.2,154.8,165.4,115.0,114.6,191.2,142.4,116.8,96.4,84.0,122.6,116.8,127.2,131.6,102.8,105.8,123.2,147.8,102.6,121.2,90.0,100.0,92.0,124.6,90.8,85.2,113.2,108.0,94.4,83.8,103.0,82.4,108.8,83.0,78.4,90.4,92.6,68.8,97.2,81.0,83.8,85.8,89.4,73.8,78.4,68.0,86.4,85.2,84.4,82.4,80.2,82.8,72.0,78.4,80.8,74.4,73.4,85.0,62.8,73.4

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,
Policy Iteration,
Q Learning,163.0,242.6,215.6,208.8,268.2,247.8,253.6,278.2,283.0,308.0,324.8,332.4,351.0,343.4,370.6,372.8,376.0,387.8,396.8,413.6,450.8,442.2,480.8,475.4,470.4,489.6,491.0,474.0,480.4,500.2,512.6,516.8,520.6,506.0,522.0,543.2,524.8,534.2,544.4,536.2,554.6,539.4,570.8,583.8,573.4,566.4,606.6,603.2,622.4,613.0,600.8,608.8,624.6,617.6,625.4,636.4,615.2,638.6,631.6,625.6,643.2,652.6,651.0,641.6,636.4,650.8,645.8,662.6,690.2,669.8,672.6,684.4,654.2,668.2,708.8,676.0,709.6,696.4,666.6,682.6,688.4,691.2,683.8,717.8,692.8,718.4,750.0,712.2,693.6,707.4,727.4,765.4,736.8,730.6,720.4,724.6,740.8,735.4,743.8,717.6

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration Rewards,
Policy Iteration Rewards,
Q Learning Rewards,-1339.6,-965.6,-482.8,-707.6,-931.2,-636.6,-274.6,-411.2,-237.8,-392.6,-351.6,-362.2,-291.0,-392.4,-384.6,-200.0,-175.2,-176.8,-178.6,-138.0,-127.2,-71.0,-229.0,-111.8,-182.2,-115.2,-133.0,-93.6,-66.6,-51.6,-167.6,-93.0,-95.0,-54.4,-69.0,-21.6,-52.4,-100.6,-55.6,-29.8,-21.2,-52.8,-63.4,-13.0,-12.6,-89.2,-40.4,-14.8,5.6,18.0,-20.6,-14.8,-25.2,-29.6,-0.8,-3.8,-21.2,-45.8,-0.6,-19.2,12.0,2.0,10.0,-22.6,11.2,16.8,-11.2,-6.0,7.6,18.2,-1.0,19.6,-6.8,19.0,23.6,11.6,9.4,33.2,4.8,21.0,18.2,16.2,12.6,28.2,23.6,34.0,15.6,16.8,17.6,19.6,21.8,19.2,30.0,23.6,21.2,27.6,28.6,17.0,39.2,28.6


